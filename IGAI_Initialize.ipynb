{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup \n",
    "from bs4 import BeautifulSoup \n",
    "import json\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import os.path\n",
    "import math\n",
    "import re\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "\n",
    "from selenium.webdriver.support.ui import WebDriverWait     \n",
    "from selenium.webdriver.common.by import By     \n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'warn', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if not sys.warnoptions:\n",
    "#    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "#warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "np.seterr(all='warn')\n",
    "#warnings.simplefilter(\"always\")\n",
    "#warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_lists_path = path + 'users_lists/'\n",
    "\n",
    "users_file = path + 'users'\n",
    "users_ext_file = path + 'users_ext'\n",
    "\n",
    "relationstrk_file = path + 'relationstrk'\n",
    "likestrk_file = path + 'likestrk'\n",
    "relations_file = path + 'relations'\n",
    "relations_ext_file = path + 'relations_ext'\n",
    "\n",
    "vipuserstrk_file = path + 'vipuserstrk'\n",
    "\n",
    "status_file = path + 'status'\n",
    "status_likes_file = path + 'status_likes'\n",
    "\n",
    "pwd_file = path + 'pwd'\n",
    "\n",
    "\n",
    "user_info_ext_file = path + 'user_info_ext'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "usercolumntypes = {   'userid': object\n",
    "                    , 'username': object\n",
    "                    , 'full_name': object\n",
    "                    , 'profile_pic_url': object\n",
    "                    , 'is_private': bool\n",
    "                    , 'is_verified': bool\n",
    "                    , 'last_update_time_basic': str\n",
    "                    , 'is_business_account': object\n",
    "                    , 'posts': float\n",
    "                    , 'followers': float\n",
    "                    , 'followed': float\n",
    "                    , 'highlighted_stories': float\n",
    "                    , 'igtv': float\n",
    "                    , 'collections': float\n",
    "                    , 'connected_fb_page': float\n",
    "                    , 'biography': object\n",
    "                    , 'is_joined_recently': object\n",
    "                    , 'business_category_name': object\n",
    "                    , 'overall_category_name': float\n",
    "                    , 'category_enum': object\n",
    "                    , 'has_ar_effects': object\n",
    "                    , 'has_clips': object\n",
    "                    , 'has_guides': object\n",
    "                    , 'has_channel': object\n",
    "                    , 'external_url': object\n",
    "                    , 'profile_pic_url_hd': object\n",
    "                    , 'external_url_linkshimmed': object\n",
    "                    , 'last_update_time_info': str\n",
    "                  }\n",
    "\n",
    "#para hacer esto con otras tablas ayuda mucho esto (ojo! no usar ):\n",
    "#users.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Screenshot function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining screenshot function\n",
    "def screenCap():\n",
    "    millis = int(round(time.time() * 1000))\n",
    "    imgName=millis\n",
    "    driver.save_screenshot('Screens/'+\"test\"+str(imgName)+\".png\")\n",
    "    img=mpimg.imread('Screens/'+\"test\"+str(imgName)+\".png\")\n",
    "    imgplot = plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Login function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def do_Login(username,password):\n",
    "    driver.get('https://www.instagram.com/accounts/login/')\n",
    "    time.sleep(1)\n",
    "    driver.find_element_by_xpath(\"//*[text()='Aceptar']\").click()\n",
    "    driver.find_element_by_name(\"username\").send_keys(username)\n",
    "    driver.find_element_by_name(\"password\").send_keys(password)\n",
    "    time.sleep(1)\n",
    "    driver.find_element_by_xpath(\"//*[text()='Iniciar sesiÃ³n']\").click()\n",
    "    time.sleep(5)\n",
    "    driver.find_element_by_xpath(\"//*[text()='Ahora no']\").click()\n",
    "    time.sleep(5)\n",
    "    driver.find_element_by_xpath(\"//*[text()='Ahora no']\").click()\n",
    "\n",
    "    #screenCap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open IG function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchAndPrint(searchText):\n",
    "    driver.find_element_by_xpath(\"//*[@placeholder='Busca']\").clear()\n",
    "    driver.find_element_by_xpath(\"//*[@placeholder='Busca']\").send_keys(searchText)\n",
    "    #screenCap()\n",
    "    time.sleep(3)\n",
    "    ele=driver.find_element_by_xpath(\"//*[@class='fuqBx']\")\n",
    "    results=ele.find_elements_by_tag_name(\"a\")\n",
    "    l=[]\n",
    "    for i in results:\n",
    "        #Excluding Hashtags\n",
    "        if \"#\" not in i.text:\n",
    "            l.append(i.text.partition('\\n')[0])\n",
    "            print(i.text.partition('\\n')[0],end=\" , \")\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search and Open Profile Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchAndOpenProfile(profileName):\n",
    "    driver.find_element_by_xpath(\"//*[@placeholder='Busca']\").clear()\n",
    "    driver.find_element_by_xpath(\"//*[@placeholder='Busca']\").send_keys(profileName)\n",
    "    time.sleep(3)\n",
    "    #screenCap()\n",
    "    ele=driver.find_element_by_xpath(\"//*[@class='fuqBx']\")\n",
    "    results=ele.find_elements_by_tag_name(\"a\")\n",
    "    for i in results:\n",
    "        if \"#\" not in i.text:\n",
    "            i.click()\n",
    "            #print(i.text.partition('\\n')[0]+\" Profile Opened\")\n",
    "            #plt.show()\n",
    "            #driver.switch_to.default_content().click()\n",
    "            #driver..click()\n",
    "            #ActionChains(driver).move_to_element(driver.find_element_by_xpath(\"//div[@class='aIYm8 coreSpriteSearchClear']\")).click().perform()\n",
    "            driver.find_element_by_xpath(\"//*[@class='aIYm8 coreSpriteSearchClear']\").click()\n",
    "            break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Profile Stats (Selenium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profileBasicInfo(profileName):\n",
    "    #searchAndOpenProfile(profileName)\n",
    "    url = 'https://www.instagram.com/' + profileName\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "    ele=driver.find_element_by_xpath(\"//*[@class='k9GMp ']\")\n",
    "    head=ele.find_elements_by_tag_name(\"span\")\n",
    "    posts = int(head[1].text.replace(\".\",\"\")) \n",
    "    followers_rounded = head[2].text\n",
    "    followers_exact = head[2].get_attribute('title') \n",
    "    if followers_exact == '':\n",
    "        account_type = 'private'\n",
    "        k = 1 #the structure is a litle bit different\n",
    "    else:\n",
    "        account_type = 'public'\n",
    "        k = 0\n",
    "    followers_rounded = head[2+k].text\n",
    "    followers_exact = int(head[2+k].get_attribute('title').replace(\".\",\"\")) \n",
    "    followed = int(head[3+2*k].text.replace(\".\",\"\"))\n",
    "    \n",
    "    #convert followers rounded into a number\n",
    "    multiply = 1\n",
    "    if followers_rounded.find(\"k\") >= 0:\n",
    "        multiply = 1000\n",
    "    elif followers_rounded.find(\"mm\") >=0:\n",
    "        multiply = 1000000\n",
    "    followers_rounded = int(float(re.sub('[a-z]','', followers_rounded).replace('.','').replace(',','.'))*multiply)\n",
    "    \n",
    "    #Print results\n",
    "    print('Stats of ' + profileName + ' (' + account_type + ' account)')\n",
    "    print('----------------------------------------------')\n",
    "    print('posts: ' + str(posts))\n",
    "    if account_type == 'private' and multiply > 1:\n",
    "        print('followers: ' + str(followers_rounded) + ' (cannot know exact number)')\n",
    "    elif multiply == 1:\n",
    "        print('followers: ' + str(followers_rounded))\n",
    "    else:\n",
    "        print('followers: ' + str(followers_rounded) + ' (exactly ' + str(followers_exact) +')')\n",
    "    print('following: ' + str(followed))\n",
    "    \n",
    "    #build output dataframe\n",
    "    data = {'username': [profileName],\n",
    "            'posts': [posts], \n",
    "            'followers_rounded': [followers_rounded],\n",
    "            'followers_exact': [followers_exact],\n",
    "            'followed': [followed]\n",
    "           } \n",
    "    df = pd.DataFrame(data) \n",
    "    return df\n",
    "\n",
    "#\n",
    "#'-nal3 '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Info (Soap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UserInfo(username):\n",
    "\n",
    "    url = 'https://www.instagram.com/' + username + '/?__a=1'\n",
    "    edge = 'user'\n",
    "\n",
    "    driver.get(url)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    s = json.loads(soup.text)\n",
    "    #user_info = pd.json_normalize(s)\n",
    "    user_info = pd.json_normalize(s[\"graphql\"][edge])\n",
    "\n",
    "    user_info = user_info[['id',  'username', \"full_name\",  'is_private',  'is_verified', 'is_business_account',\n",
    "                             'edge_owner_to_timeline_media.count', 'edge_followed_by.count', 'edge_follow.count', \n",
    "                             'highlight_reel_count',  'edge_felix_video_timeline.count', 'edge_media_collections.count',\n",
    "                             'connected_fb_page', \"biography\", 'is_joined_recently', 'business_category_name',\n",
    "                             'overall_category_name', 'category_enum', \n",
    "                             'has_ar_effects', 'has_clips', 'has_guides', 'has_channel',\n",
    "                             'external_url', 'profile_pic_url', 'profile_pic_url_hd', 'external_url_linkshimmed'\n",
    "                            ]]\n",
    "    \n",
    "    user_info = user_info.rename(columns = {'id': 'userid',\n",
    "                                            'edge_owner_to_timeline_media.count': 'posts',\n",
    "                                            'edge_followed_by.count': 'followers',\n",
    "                                            'edge_follow.count': 'followed',                                       \n",
    "                                            'highlight_reel_count': 'highlighted_stories',\n",
    "                                            'edge_felix_video_timeline.count': 'igtv',\n",
    "                                            'edge_media_collections.count': 'collections'\n",
    "                                           })\n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "    user_info[\"inserted_time\"] = now\n",
    "    user_info[\"last_update_time\"] = now\n",
    "    \n",
    "    #Update users table\n",
    "    if os.path.isfile(user_info_ext_file):\n",
    "        user_info.to_csv(user_info_ext_file, mode='a', header=False, index = False) #append\n",
    "    else:\n",
    "        user_info.to_csv(user_info_ext_file, mode='w', header=True , index = False) #create/overwrite\n",
    "        \n",
    "    updateUsersTable(user_info_ext_file, users_file)\n",
    "    \n",
    "    user_info = user_info.drop(columns = [\"last_update_time\"])\n",
    "    \n",
    "    #Save tracking of this user in vipuserstrk\n",
    "    UpdateVipUsers(user_info)\n",
    "    \n",
    "    return user_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract followers of user (Selenium- slow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def followersList(handleName,numberOfFollowers):\n",
    "    searchAndOpenProfile(handleName)\n",
    "    time.sleep(3)\n",
    "    handle='/'+handleName+'/followers/'\n",
    "    # Making dynamic xpath\n",
    "    xpath=\"//*[@href='\"+handle+\"'\"+']' \n",
    "    driver.find_element_by_xpath(xpath).click()\n",
    "    time.sleep(2)\n",
    "    i=0\n",
    "    j=0\n",
    "    while i<=numberOfFollowers+j:\n",
    "        time.sleep(0.5)\n",
    "        element=driver.find_element_by_xpath(\"//*[@class='PZuss']\")\n",
    "        results=element.find_elements_by_tag_name(\"a\")\n",
    "        \n",
    "        x=[]\n",
    "        for k in results:\n",
    "            if k.text.strip()!='':\n",
    "                x.append(k.text.strip())\n",
    "        \n",
    "        if (len(x)<numberOfFollowers):\n",
    "            followerWindow = driver.find_element_by_xpath(\"//*[@class='isgrP']\")\n",
    "            elements=followerWindow.find_elements_by_tag_name(\"a\")\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView();\", elements[i])\n",
    "            i=i+1\n",
    "            j=j+1\n",
    "        else:\n",
    "        \n",
    "            element_new=driver.find_element_by_xpath(\"//*[@class='PZuss']\")\n",
    "            results_new=element.find_elements_by_tag_name(\"a\")\n",
    "            l=[]\n",
    "            count=0\n",
    "            for r in results_new:\n",
    "                if r.text.strip()!='':\n",
    "                    count=count+1\n",
    "                    l.append(r.text.strip())\n",
    "                    if count==numberOfFollowers:\n",
    "                        #driver.find_element_by_xpath(\"//*[@class='wpO6b ']\").click()\n",
    "                        driver.get('https://www.instagram.com/'+ handleName)\n",
    "                        return l\n",
    "                        break\n",
    "            print(l)\n",
    "            driver.get('https://www.instagram.com/'+ handleName)\n",
    "            return l\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Followers of user (limit > 20k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFollowers(identificator, process, max_status_hours, requests, risk_level):\n",
    "    #max_status_hours: maximum hours to look backwards in status log table\n",
    "    #process can be followers or followed or hashtags\n",
    "    \n",
    "    #user data\n",
    "    if process == 'followers' or process == 'followed':\n",
    "        username = identificator\n",
    "        user_info = UserInfo(username)\n",
    "        userid = user_info.userid[0]\n",
    "        identificatorvar = 'username'\n",
    "        identificatorid = userid\n",
    "        urlidvar = 'id'\n",
    "        \n",
    "    #Parameters that depend on the process    \n",
    "    if process == 'hashtags':\n",
    "        query_hash = '???'\n",
    "        edge = '???'\n",
    "    elif process == 'likes':\n",
    "        status_file_local = status_likes_file\n",
    "        relationstrk_file_local = likestrk_file\n",
    "        query_hash = 'd5d763b1e2acf209d62d22d184488e57'\n",
    "        edge = 'edge_liked_by'\n",
    "        edge_parent = 'shortcode_media'\n",
    "        identificatorvar = 'shortcode'\n",
    "        identificatorid = identificator\n",
    "        urlidvar = 'shortcode'\n",
    "        instances = 1000\n",
    "    elif process == 'followers':\n",
    "        status_file_local = status_file\n",
    "        relationstrk_file_local = relationstrk_file\n",
    "        query_hash = 'c76146de99bb02f6415203be841dd25a'\n",
    "        edge = 'edge_followed_by'\n",
    "        edge_parent = 'user'\n",
    "        instances = user_info.followers[0]\n",
    "    else: #'followed'\n",
    "        status_file_local = status_file\n",
    "        relationstrk_file_local = relationstrk_file\n",
    "        query_hash = 'd04b0a864b4b54837c0d870b0e77e076'\n",
    "        edge = 'edge_follow'\n",
    "        edge_parent = 'user'\n",
    "        instances = user_info.followed[0]\n",
    "        process = 'followed'\n",
    "\n",
    "    process_name = process + \"_extraction\"\n",
    "        \n",
    "    #parameters\n",
    "    \n",
    "    sr = 1  #sleep ratio\n",
    "    #modify sleep ratio as a function of risk_level (which should be between 0 and 1)\n",
    "    if risk_level > 1: \n",
    "        sr = 0.5\n",
    "    elif risk_level < 0:\n",
    "        sr = 2\n",
    "    else:\n",
    "        sr = 2 - 1.5*risk_level\n",
    "    if instances > 10000:\n",
    "        sr = 2*sr\n",
    "    print(\"Sleep ratio: \" + str(round(sr,2)))\n",
    "    \n",
    "    max_retries = 5           # if gets rate limit  \n",
    "    results_num = 50          # how many users requests in each bulk (admits max 50 but sometimes downloads less ~48)\n",
    "    short_sleep = 2*sr        # every bulk\n",
    "    medium_sleep = 5*sr       # every 10 bulks\n",
    "    long_sleep = 150*sr       # every 200 bulks or too_many_requests_message\n",
    "    atpf = 48/short_sleep     # atpf = average time per follower\n",
    "    \n",
    "    #initialize void data frames\n",
    "    users = pd.DataFrame([])\n",
    "    users_ext = pd.DataFrame([])\n",
    "    relations = pd.DataFrame([])\n",
    "    relations_ext = pd.DataFrame([])\n",
    "    \n",
    "    #first url\n",
    "    url = 'https://www.instagram.com/graphql/query/?query_hash=' + query_hash + '&variables=%7B%22id%22%3A%22' + str(identificatorid) + '%22%2C%22include_reel%22%3Atrue%2C%22fetch_mutual%22%3Afalse%2C%22first%22%3A'+ str(results_num) + '%7D'\n",
    "    url = 'https://www.instagram.com/graphql/query/?query_hash=' + query_hash + '&variables=%7B%22' + urlidvar + '%22%3A%22' + str(identificatorid) + '%22%2C%22include_reel%22%3Atrue%2C%22fetch_mutual%22%3Afalse%2C%22first%22%3A'+ str(results_num) + '%7D'\n",
    "\n",
    "    ins_rem = instances  #instances remaining \n",
    "    ins_ext = 0          #instances extracted \n",
    "    \n",
    "    #Load last status\n",
    "    if os.path.isfile(status_file_local):\n",
    "        status = pd.read_csv(status_file_local)\n",
    "        print('Loaded file' + status_file_local)\n",
    "        #convert inserted_time to datetime type, since read_csv reads as string\n",
    "        status['inserted_time'] =  status['inserted_time'].apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S.%f'))\n",
    "        #keep only last max_status_hours & only desired process & user analyzed\n",
    "        status = status[status.inserted_time > datetime.datetime.now() - datetime.timedelta(hours=max_status_hours)]\n",
    "        status = status[status.process == process_name]\n",
    "        status = status[status[identificatorvar] == identificator]\n",
    "        if len(status) > 0:\n",
    "            status = status[status.inserted_time == max(status.inserted_time)]\n",
    "            if not(status.is_finished.iloc[0]):\n",
    "                url = status.url.iloc[0]\n",
    "                ins_ext = int(round(instances*status.percent_done.iloc[0]/100,0))  # extracted followers\n",
    "                ins_rem  = instances - ins_ext # remaining followers\n",
    "                print('Taking last status... ' + str(round(status.percent_done.iloc[0]))+ \"% already extracted. Let's continue.\")\n",
    "            else:\n",
    "                print(identificator + ' already extracted in the last ' + str(max_status_hours) + ' hours')\n",
    "                return 0 \n",
    "\n",
    "    #Print time aproximation\n",
    "    print('\\nExtracting ' + str(ins_rem) + ' ' + process + ' of ' + identificator + ' (total: ' + str(instances) + \n",
    "          ') will take ' + str(math.floor(ins_rem/(atpf*60))) + ' minutes ' + str(int((ins_rem/atpf % 60))) + ' seconds aproximately.')\n",
    "    \n",
    "    \n",
    "    #Initialize some variables before loop\n",
    "    has_next_page = True\n",
    "    \n",
    "    print('Extracted: ' + str(ins_ext), end='')\n",
    "\n",
    "\n",
    "    while has_next_page:\n",
    "    \n",
    "        #Go to url and extract text\n",
    "        time.sleep(short_sleep)\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        s = json.loads(soup.text)\n",
    "        \n",
    "        #rate limit control retry\n",
    "        retries = 1\n",
    "        while \"message\" in s and retries <= max_retries:\n",
    "            print('\\n' + s[\"message\"])\n",
    "            time.sleep(long_sleep*retries)\n",
    "            driver.get(url)\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            s = json.loads(soup.text)\n",
    "            retries = retries + 1\n",
    "        \n",
    "        #get next iterarion params\n",
    "        has_next_page = s[\"data\"][edge_parent][edge][\"page_info\"][\"has_next_page\"]\n",
    "        end_cursor = s[\"data\"][edge_parent][edge][\"page_info\"][\"end_cursor\"]\n",
    "        if has_next_page:\n",
    "            #url = 'https://www.instagram.com/graphql/query/?query_hash=' + query_hash + '&variables=%7B%22id%22%3A%22' + str(userid) + '%22%2C%22include_reel%22%3Atrue%2C%22fetch_mutual%22%3Afalse%2C%22first%22%3A' + str(results_num) + '%2C%22after%22%3A%22' + end_cursor + '%22%7D'\n",
    "            url = 'https://www.instagram.com/graphql/query/?query_hash=' + query_hash + '&variables=%7B%22' + urlidvar + '%22%3A%22' + str(identificatorid) + '%22%2C%22include_reel%22%3Atrue%2C%22fetch_mutual%22%3Afalse%2C%22first%22%3A'+ str(results_num) + '%2C%22after%22%3A%22' + end_cursor + '%22%7D'\n",
    "        #download\n",
    "        if s[\"data\"][edge_parent][edge][\"edges\"] != []:\n",
    "            users_ext = pd.json_normalize(s[\"data\"][edge_parent][edge][\"edges\"])\n",
    "            colnames = list(users_ext.columns)\n",
    "            colnames = [sub.replace('node.', '') for sub in colnames] \n",
    "            colnames = [sub.replace('.', '_') for sub in colnames] \n",
    "            users_ext.columns = colnames\n",
    "            now = datetime.datetime.now()\n",
    "            users_ext.loc[:,\"inserted_time\"] = now\n",
    "            users_ext.loc[:,\"last_update_time\"] = now\n",
    "            users_ext = users_ext[[\"id\", \"username\", \"full_name\", \"profile_pic_url\", \"is_private\", \"is_verified\", \"inserted_time\",\"last_update_time\"]]\n",
    "            users_ext.rename(columns = {'id': 'userid'}, inplace = True)\n",
    "\n",
    "            #save users_ext_file\n",
    "            if os.path.isfile(users_ext_file):\n",
    "                users_ext.to_csv(users_ext_file, mode='a', header=False, index = False) #append\n",
    "            else:\n",
    "                users_ext.to_csv(users_ext_file, mode='w', header=True, index = False) #create/overwrite\n",
    "\n",
    "            #UsersRelations\n",
    "            relations_ext = users_ext[[\"userid\", \"username\", \"inserted_time\",\"last_update_time\"]]\n",
    "            relations_ext.loc[:,\"process\"] = process_name\n",
    "            if process == 'likes':\n",
    "                relations_ext.loc[:,'shortcode'] = identificator\n",
    "                relations_ext.loc[:,\"status\"] = \"likes\"\n",
    "                relations_ext = relations_ext[[\"status\", \"shortcode\", \"userid\", \"username\",\"inserted_time\",\"process\"]]\n",
    "            elif process == 'followers':\n",
    "                relations_ext = relations_ext.rename(columns = {'userid': 'followed_by_userid'})\n",
    "                relations_ext = relations_ext.rename(columns = {'username': 'followed_by_username'})\n",
    "                relations_ext.loc[:,\"userid\"] = userid\n",
    "                relations_ext.loc[:,\"username\"] = username\n",
    "                relations_ext.loc[:,\"status\"] = \"following\"\n",
    "                relations_ext = relations_ext[[\"status\",\"userid\", \"username\",\"followed_by_userid\",\"followed_by_username\",\"inserted_time\",\"process\"]]\n",
    "            elif process == 'followed':\n",
    "                relations_ext.loc[:,'followed_by_userid'] = userid\n",
    "                relations_ext.loc[:,'followed_by_username'] = username\n",
    "                relations_ext.loc[:,\"status\"] = \"following\"\n",
    "                relations_ext = relations_ext[[\"status\",\"userid\", \"username\",\"followed_by_userid\",\"followed_by_username\",\"inserted_time\",\"process\"]]\n",
    "\n",
    "            #save users_ext_file\n",
    "            if os.path.isfile(relationstrk_file_local):\n",
    "                relations_ext.to_csv(relationstrk_file_local, mode='a', header=False, index = False) #append\n",
    "            else:\n",
    "                relations_ext.to_csv(relationstrk_file_local, mode='w', header=True, index = False) #create\n",
    "\n",
    "            #concat dfs\n",
    "            relations = pd.concat([relations_ext, relations], ignore_index=True)\n",
    "\n",
    "        #save status\n",
    "        now = datetime.datetime.now()\n",
    "        if process == 'followers' or process == 'followed':\n",
    "            d = {'userid': [userid], 'username': [username], 'process': [process_name], 'is_finished':[not(has_next_page)],\n",
    "                 'percent_done': [100*(ins_ext + len(relations.index))/instances], 'inserted_time': [now],\n",
    "                 'inserted_date': [now.date()], 'end_cursor':[ end_cursor], 'url': [url]}\n",
    "        elif process == 'likes':\n",
    "            d = {'shortcode': [identificator], 'process': [process_name], 'is_finished':[not(has_next_page)],\n",
    "                 'percent_done': [100*(ins_ext + len(relations.index))/instances], 'inserted_time': [now],\n",
    "                 'inserted_date': [now.date()], 'end_cursor':[ end_cursor], 'url': [url]}\n",
    "        status = pd.DataFrame(data=d) \n",
    "        if os.path.isfile(status_file_local):\n",
    "            status.to_csv(status_file_local, mode='a', header=False, index = False) #append\n",
    "        else:\n",
    "            status.to_csv(status_file_local, mode='w', header=True, index = False) #create/overwrite\n",
    "        \n",
    "        #print users extracted\n",
    "        print(' -> ' + str( ins_ext + len(relations.index)), end='')\n",
    "        \n",
    "        requests = requests + 1\n",
    "        \n",
    "        #Sleep\n",
    "        #if requests % 10 == 0:\n",
    "        #    time.sleep(medium_sleep)\n",
    "            \n",
    "        #if requests % 100 == 0:\n",
    "        #    time.sleep(long_sleep)\n",
    "        \n",
    "        if requests % 200 == 0:\n",
    "            time.sleep(long_sleep)\n",
    "    \n",
    "\n",
    "    print('\\n')\n",
    "    updateUsersTable(users_ext_file, users_file)\n",
    "    if process == 'followers' or process == 'followed':\n",
    "        updateRelationsTable(relationstrk_file, relations_file)    \n",
    "\n",
    "    return requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Users Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateUsersTable(users_ext_file, users_file):\n",
    "    # This function updates users table (stored in users_file) with data extracted (stored in users_ext_file) \n",
    "    # INPUT\n",
    "    # users_ext_file: csv file with data of users extracted\n",
    "    # users_file: csv file with data of users\n",
    "    # OUTPUT\n",
    "    # No output but users_file is updated and users_ext_file removed\n",
    "    \n",
    "\n",
    "       #'userid', 'username', 'full_name', 'profile_pic_url', 'is_private',\n",
    "       #'is_verified', 'last_update_time', 'is_business_account', 'posts',\n",
    "       #'followers', 'followed', 'highlighted_stories', 'igtv', 'collections',\n",
    "       #'connected_fb_page', 'biography', 'is_joined_recently',\n",
    "       #'business_category_name', 'overall_category_name', 'category_enum',\n",
    "       #'has_ar_effects', 'has_clips', 'has_guides', 'has_channel',\n",
    "       #'external_url', 'profile_pic_url_hd', 'external_url_linkshimmed',\n",
    "       #'inserted_time'\n",
    "    \n",
    "    #Define blocks of data to be updated independently\n",
    "    pk = list(['userid', 'username'])\n",
    "    basic = list([\"full_name\", \"profile_pic_url\", \"is_private\", \"is_verified\", \"last_update_time_basic\"])\n",
    "    userinfo = list(['is_business_account', 'posts','followers', 'followed', 'highlighted_stories', 'igtv', 'collections',\n",
    "                     'connected_fb_page', 'biography', 'is_joined_recently','business_category_name', 'overall_category_name', \n",
    "                     'category_enum','has_ar_effects', 'has_clips', 'has_guides', 'has_channel','external_url', 'profile_pic_url_hd', \n",
    "                     'external_url_linkshimmed', \"last_update_time_info\"])\n",
    "    \n",
    "    blocks = pd.DataFrame({'block_name': ['basic', 'userinfo'],\n",
    "                           'lastupdatetime': ['last_update_time_basic', 'last_update_time_info'],\n",
    "                           'block_columns': [basic, userinfo]\n",
    "                         })\n",
    "\n",
    "    #If both CSV files exist, then concatenate, deduplicate and remove user_ext\n",
    "    if os.path.isfile(users_ext_file):\n",
    "        users_ext = pd.read_csv(users_ext_file, dtype=usercolumntypes)\n",
    "        print('Loaded file users_ext.csv with ' + str(len(users_ext.index)) + ' records.')\n",
    "        if os.path.isfile(users_file):\n",
    "            users = pd.read_csv(users_file, dtype=usercolumntypes)\n",
    "            print('Loaded file users.csv with ' + str(len(users.index))  + ' records.')\n",
    "        else:\n",
    "            users = pd.DataFrame([],columns = pk + basic + userinfo)\n",
    "\n",
    "        #merge both dfs\n",
    "        users_ext['last_update_time_basic'] = users_ext['last_update_time']\n",
    "        users_ext['last_update_time_info'] = users_ext['inserted_time']\n",
    "        users = pd.concat([users_ext, users])\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Create finaldf\n",
    "        final_df = users[pk].drop_duplicates()\n",
    "\n",
    "        #Lets update each block of informations\n",
    "        for i in range(len(blocks)):\n",
    "            \n",
    "            #get variables from this block\n",
    "            block_name = blocks[\"block_name\"].iloc[i]\n",
    "            lastupdatetime = blocks[\"lastupdatetime\"].iloc[i]\n",
    "            block_columns = blocks[\"block_columns\"].iloc[i]\n",
    "            \n",
    "            #Take most updated row for each userid\n",
    "            sub_df = users[pk + block_columns]\n",
    "            sub_df = sub_df.sort_values(by = lastupdatetime, ascending=False)\n",
    "            sub_df = sub_df.drop_duplicates(subset = pk, keep='first')\n",
    "            \n",
    "            #merge to final_df\n",
    "            final_df = pd.merge(final_df, sub_df, on = pk, how='left')\n",
    "\n",
    "        #remove updated file and overwrite users\n",
    "        print('Table users updated, now it has ' + str(len(final_df.index)) + ' records.')\n",
    "        final_df.to_csv(users_file, index = False)\n",
    "        print('Saved csv file: ' + users_file)\n",
    "        os.remove(users_ext_file)\n",
    "        print('Removed csv file: ' + users_ext_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Relations Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateRelationsTable(input_trk_file, output_dim_file):\n",
    "    # This function updates users table (stored in users_file) with data extracted (stored in users_ext_file) \n",
    "    # INPUT\n",
    "    # users_ext_file: csv file with data of users extracted\n",
    "    # users_file: csv file with data of users\n",
    "    # OUTPUT\n",
    "    # No output but users_file is updated and users_ext_file removed\n",
    "    \n",
    "    #If both CSV files exist, then concatenate, deduplicate and remove user_ext\n",
    "    if os.path.isfile(input_trk_file):\n",
    "        ext = pd.read_csv(input_trk_file)\n",
    "        print('Loaded file ' + input_trk_file + ' with ' + str(len(ext.index)) + ' records.')\n",
    "        ext.loc[:,\"last_update_time\"] = ext.loc[:,\"inserted_time\"]\n",
    "        ext.loc[:,\"last_following_time\"] = ext.loc[:,\"inserted_time\"]\n",
    "        \n",
    "        if os.path.isfile(output_dim_file):\n",
    "            df = pd.read_csv(output_dim_file)\n",
    "            print('Loaded file ' + output_dim_file + ' with ' + str(len(df.index))  + ' records.')\n",
    "            #merge both dfs\n",
    "            df = pd.concat([ext, df])\n",
    "        else:\n",
    "            df = ext\n",
    "\n",
    "        #Create new dfs\n",
    "        df_inserted_time = df[[\"userid\", 'followed_by_userid', \"inserted_time\"]]\n",
    "        df_lastfwg_time = df[df.status == 'following'][[\"userid\", 'followed_by_userid', \"last_following_time\"]]\n",
    "\n",
    "\n",
    "        #Keep last status of each relation\n",
    "        df = df.sort_values(by=\"last_update_time\", ascending=False) \n",
    "        df = df.drop_duplicates(subset=['userid', 'followed_by_userid'], keep='first')\n",
    "        df = df.drop(['inserted_time','last_following_time'],  axis='columns')\n",
    "        \n",
    "\n",
    "        #first-time-seen relation\n",
    "        df_inserted_time = df_inserted_time.sort_values(by=\"inserted_time\", ascending=True)\n",
    "        df_inserted_time = df_inserted_time.drop_duplicates(subset=['userid', 'followed_by_userid'], keep='first')\n",
    "        \n",
    "        #last-time-following relation\n",
    "        df_lastfwg_time = df_lastfwg_time.sort_values(by=\"last_following_time\", ascending=False)\n",
    "        df_lastfwg_time = df_lastfwg_time.drop_duplicates(subset=['userid', 'followed_by_userid'], keep='first')\n",
    "        \n",
    "        #merge\n",
    "        df = pd.merge(df, df_inserted_time, on=['userid','followed_by_userid'], how='left')\n",
    "        df = pd.merge(df, df_lastfwg_time, on=['userid','followed_by_userid'], how='left')\n",
    "        #df = df.drop('Unnamed: 0',  axis='columns')\n",
    "        print('Table relations updated, now it has ' + str(len(df.index)) + ' records.')\n",
    "        \n",
    "\n",
    "        #remove updated file and overwrite\n",
    "        df.to_csv(output_dim_file, index = False)\n",
    "        print('Saved csv file: ' + output_dim_file)\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update VipUsers Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UpdateVipUsers(user_info):\n",
    "        \n",
    "    if os.path.isfile(vipuserstrk_file):\n",
    "        user_info.to_csv(vipuserstrk_file, mode='a', header=False, index = False) #append\n",
    "    else:\n",
    "        user_info.to_csv(vipuserstrk_file, mode='w', header=True , index = False) #create/overwrite\n",
    "        \n",
    "    print('Saved csv file: ' + vipuserstrk_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Follow / Unfollow This User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def followUnfollowThisUser(username, process):\n",
    "    \n",
    "    status = 'unknown'\n",
    "    is_done = 0  #boolean to check if the action (follow or unfollow) has been done. 0=false, 1=true, -1=blocked   \n",
    "    \n",
    "    url = 'https://www.instagram.com/' + username + '/'\n",
    "    driver.get(url)\n",
    "    time.sleep(1)\n",
    "    try:\n",
    "      element = driver.find_element_by_xpath(\"//*[@class='nZSzR']\")\n",
    "    except:\n",
    "      print('@' + username + ' does not exist anymore. Action ' + process + ' skipped.')\n",
    "      return is_done\n",
    "    \n",
    "    element = element.find_elements_by_tag_name(\"button\")\n",
    "    button_text = element[0].text\n",
    "\n",
    "    \n",
    "    if process == 'follow':\n",
    "        if button_text.find(\"Seguir\") >= 0: #this includes \"Seguir tambien\"\n",
    "            element[0].click()\n",
    "            time.sleep(2)\n",
    "            #check if instagram blocked the follow\n",
    "            element = driver.find_element_by_xpath(\"//*[@class='nZSzR']\")\n",
    "            element = element.find_elements_by_tag_name(\"button\")\n",
    "            button_text = element[0].text\n",
    "            if button_text.find(\"Seguir\") >= 0:\n",
    "                print(\"THIS ACTION WAS BLOCKED BY INSTAGRAM <-------------------------------------------\")\n",
    "                print(\"@\" + account + \" did not followed @\" + username)\n",
    "                return -1\n",
    "            status = 'following'\n",
    "            print(\"@\" + account + \" started following @\" + username)\n",
    "            is_done = 1\n",
    "        else:\n",
    "            status = 'following'\n",
    "            print(\"@\" + account + \" already follows @\" + username + ' (status: ' + button_text + ')')\n",
    "            \n",
    "    elif process == 'unfollow':\n",
    "        if button_text.find(\"Solicitado\") >= 0 or button_text.find(\"Enviar mensaje\") >= 0:\n",
    "            if button_text.find(\"Solicitado\") >= 0:\n",
    "                ind = 0\n",
    "            elif button_text.find(\"Enviar mensaje\") >= 0:\n",
    "                ind = 1\n",
    "            element[ind].click()\n",
    "            unfollow_modal = driver.find_element_by_xpath(\"//*[@class='mt3GC']\")\n",
    "            unfollow_modal = unfollow_modal.find_elements_by_tag_name(\"button\")\n",
    "            unfollow_button_text = unfollow_modal[0].text\n",
    "            time.sleep(1)\n",
    "            unfollow_modal[0].click()\n",
    "            status = 'unfollowed'\n",
    "            print(\"@\" + account + \" unfollowed @\" + username)\n",
    "            is_done = 1\n",
    "        else:\n",
    "            print(\"@\" + account + \" does not follow @\" + username + ' (no action required)')\n",
    "            \n",
    "    #build output dataframe\n",
    "    if status != 'unknown':\n",
    "        users = pd.read_csv(users_file, dtype=usercolumntypes)\n",
    "        userid = users[users.username == username][\"userid\"].iloc[0]\n",
    "        account_id = users[users.username == account][\"userid\"].iloc[0]\n",
    "\n",
    "        data = {'status': [status],\n",
    "                'userid': [userid],\n",
    "                'username': [username],\n",
    "                'followed_by_userid': [account_id], \n",
    "                'followed_by_username': [account],\n",
    "                'inserted_time': [datetime.datetime.now()],\n",
    "                'process':[\"bot_\" + process]\n",
    "               } \n",
    "        relationstrk = pd.DataFrame(data) \n",
    "        \n",
    "        if os.path.isfile(relationstrk_file):\n",
    "            relationstrk.to_csv(relationstrk_file, mode='a', header=False, index = False) #append\n",
    "        else:\n",
    "            relationstrk.to_csv(relationstrk_file, mode='w', header=True , index = False) #create/overwrite\n",
    "\n",
    "        print('Saved csv file: ' + relationstrk_file)\n",
    "\n",
    "    return is_done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Follow/Unfollow list of users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def followUnfollowTheseUsers(username_list, process, max_requests, popularity_ratio):\n",
    "    # when popularity_ratio (followers/followed) set at 1.5 it means you don't want to follow/unfollow \n",
    "    # users with high popularity. \n",
    "    \n",
    "    #Check how many times this process has been performed today\n",
    "    relationstrk = pd.read_csv(relationstrk_file)\n",
    "    relations_account = relationstrk[relationstrk.followed_by_username == account]\n",
    "    actions_account = relations_account[relations_account.process == 'bot_' + process]    \n",
    "    actions_account['inserted_time'] = actions_account['inserted_time'].apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S.%f'))\n",
    "    actions_account_6h = actions_account[actions_account.inserted_time > datetime.datetime.now() - datetime.timedelta(hours=6)]\n",
    "    requests = len(actions_account_6h.iloc[:,0])\n",
    "    \n",
    "    #load whithelisted and blacklisted users for this account\n",
    "    whitelist =  pd.read_csv(users_lists_path + account + '_whitelist') \n",
    "    whitelist = list(whitelist[\"username\"])\n",
    "    blacklist =  pd.read_csv(users_lists_path + account + '_blacklist') \n",
    "    blacklist = list(blacklist[\"username\"])\n",
    "        \n",
    "    \n",
    "    for username in username_list:\n",
    "        \n",
    "        #initialize parameters\n",
    "        is_done = 0\n",
    "        user_followed_before = 0\n",
    "        is_too_popular = False\n",
    "        \n",
    "        # check if number of requests was exceeded\n",
    "        if requests >= max_requests:\n",
    "            print('Max_requests exceeded: ' + str(requests))\n",
    "            break\n",
    "        \n",
    "            \n",
    "        #check if user should skip action\n",
    "        if process == 'follow':\n",
    "            user_followed_before = len(relations_account[relations_account.username == username].iloc[:,0])\n",
    "            is_blacklisted_user = username in blacklist\n",
    "        elif process == 'unfollow':\n",
    "            is_whitelisted_user = username in whitelist\n",
    "\n",
    "        #follow or unfollow the user\n",
    "        if process == 'follow' and user_followed_before >= 1:\n",
    "            #print('@' + username + ' was followed somewhen in the past')\n",
    "            0\n",
    "        elif process == 'follow' and is_blacklisted_user:\n",
    "            print('@' + username + ' was not followed since it is in the blacklist')\n",
    "        elif process == 'unfollow' and is_whitelisted_user:\n",
    "            print('@' + username + ' was not unfollowed since it is in the whitelist')       \n",
    "        else:\n",
    "            if popularity_ratio > 0: \n",
    "                #check if account wants to filter by popularity_ratio\n",
    "                #save_stdout = sys.stdout\n",
    "                #sys.stdout = open('trash', 'w')\n",
    "                user_info = UserInfo(username)\n",
    "                #sys.stdout = save_stdout\n",
    "                #time.sleep(1)\n",
    "                user_followers = user_info[\"followers\"][0]\n",
    "                user_followed = user_info[\"followed\"][0]\n",
    "                if popularity_ratio < user_followers/user_followed:\n",
    "                    is_too_popular = True\n",
    "                    print('@' + username + ' was not ' + process + 'ed since its popularity ratio is too high (' + str(round(user_followers/user_followed,2)) + ')')\n",
    "                else:\n",
    "                    is_done = followUnfollowThisUser(username, process)\n",
    "            else:\n",
    "                is_done = followUnfollowThisUser(username, process)\n",
    " \n",
    "        \n",
    "        #if action performed, count up the request\n",
    "        if is_done == 1:\n",
    "            requests = requests + 1\n",
    "            print('Request number ' + str(requests))\n",
    "            time.sleep(2)\n",
    "        elif is_done == -1:\n",
    "            print('The process automatically stopped ' + process + 'ing users since this action has been blocked by instagram')\n",
    "            print('Total accounts ' + process + 'ed: ' + str(requests))\n",
    "            break\n",
    "            \n",
    "    #before ending update relations dim table        \n",
    "    updateRelationsTable(relationstrk_file, relations_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Libraries and functions loaded correctly ' + str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REQUEST DE SCROLL DE FOTOS DENTRO DE UN PERFIL, CON INFO BASICA DE LIKES, COMENTARIOS Y NÃMERO DE FOTOS DE LA PUBLICACION\n",
    "\n",
    "#Este ejemplo es del perfil de influencerdemercadillo\n",
    "#https://www.instagram.com/graphql/query/?query_hash=003056d32c2554def87228bc3fd9668a&variables=%7B%22id%22%3A%22206189039%22%2C%22first%22%3A12%2C%22after%22%3A%22QVFCbzRNVld4NnpvYzVHbFg2aVRQQ0J6Q3k1RVFoSGV6OGtVMWdqdFZmaTcxNGtleFlENDJ2Q3dBUVlyS3oyU3ZiSUh5UE5BaXVmWWxkdDBLeDd6VGVZNA%3D%3D%22%7D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reconocimiento de imÃ¡genes (limites -> solo fans o muestreo aleatorio)\n",
    "# https://cloud.google.com/vision/                                                                      # 1000 per month\n",
    "# https://rapidapi.com/imagga/api/imagga-automated-image-tagging-and-categorization                     # 2000 per month\n",
    "# https://rapidapi.com/everypixel/api/everypixel-image-recognition                                      #  100 per day\n",
    "# https://rapidapi.com/microsoft-azure-org-microsoft-cognitive-services/api/microsoft-computer-vision3  # 5000 per month\n",
    "\n",
    "\n",
    "#buscador de apis\n",
    "# https://rapidapi.com/collection/top-image-recognition-apis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
